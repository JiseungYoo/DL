{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDompNXEuGus"
      },
      "outputs": [],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr5TINyduJwZ"
      },
      "outputs": [],
      "source": [
        "!pip install pickle5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_S7liUUrpzA"
      },
      "source": [
        "# Load packages and preprocessing- Korean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZix6jfZrpzC"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ckonlpy.tag import Twitter\n",
        "from tqdm import tqdm\n",
        "from hanspell import spell_checker\n",
        "import string\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "import math\n",
        "from sklearn.preprocessing import normalize\n",
        "import time\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "import requests\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "pd.options.display.max_colwidth = 100\n",
        "spacing = Spacing()\n",
        "tagpos= Twitter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxakrRtDrpzD",
        "outputId": "cc35058e-1335-4361-cef7-00047e11690d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning:\n",
            "\n",
            "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stopwords_csv = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()\n",
        "stopwords = [i[0] for i in stopwords_csv]\n",
        "\n",
        "tagpos.add_dictionary(['단원','차시','교과서','지도서','학습지','아이들','꾸미기','여러가지','성취기준','듣기','말하기','쓰기'], 'Noun')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(text):\n",
        "    words = text.split()\n",
        "    return len(words)"
      ],
      "metadata": {
        "id": "ToogdfKQC3lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNPc7hfVrpzD",
        "outputId": "698d6317-da77-4b99-e2f8-3a4b8228b194"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning:\n",
            "\n",
            "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def my_tokenizer(doc):\n",
        "    return [token for token, pos in tagpos.pos(doc, norm=True, stem=True) if pos in ['Noun','adverb'] and (len(token) > 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj4GSn5UrpzE"
      },
      "outputs": [],
      "source": [
        "def korean_processing(filename, rules=None):\n",
        "\n",
        "    df = pd.read_excel(filename, engine='openpyxl')\n",
        "\n",
        "    #clean text\n",
        "    df['document'] = df['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣;.!? ]\",\" \")\n",
        "    df['document'] = df['document'].str.replace('[\\s]{2,}', \" \")\n",
        "\n",
        "    #tokenization\n",
        "    df_text = []\n",
        "    for sentence in tqdm(df['document']):\n",
        "        tokenized_sentence = my_tokenizer(sentence)\n",
        "        stopwords_removed = [word for word in tokenized_sentence if word not in stopwords]\n",
        "        df_text.append(stopwords_removed)\n",
        "\n",
        "    df['tokenized'] = df_text\n",
        "\n",
        "    tokenized_words = df['tokenized'].apply(lambda x: ','.join(map(str, x)))\n",
        "    tokenized_list = tokenized_words.tolist()\n",
        "\n",
        "    df['list_token'] = tokenized_list\n",
        "\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4AHa1no6tb6"
      },
      "source": [
        "# Load packages and preprocessing- english\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnLO-x5M89sg"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
        "\n",
        "\n",
        "pd.options.display.max_colwidth = 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "tags = set([\"VERB\",\n",
        "    \"NOUN\",\n",
        "    \"PRON\",\n",
        "    \"ADJ\",\n",
        "    \"ADV\",\n",
        "    \"NUM\"])\n",
        "\n",
        "def processString(txt):\n",
        "  specialChars = \"!#$%^:;-+&*()?.,\"\n",
        "  for specialChar in specialChars:\n",
        "    txt = txt.replace(specialChar, ' ')\n",
        "  return txt.lower()\n"
      ],
      "metadata": {
        "id": "llgwLRVEDCip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcFwj_lG98j2"
      },
      "outputs": [],
      "source": [
        "filename = 'ausdf.xlsx'\n",
        "ausdf = pd.read_excel(filename, engine='openpyxl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9IJS7JH-HoY"
      },
      "outputs": [],
      "source": [
        "filename = 'ontadf.xlsx'\n",
        "onta = pd.read_excel(filename, engine='openpyxl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai7abF0dBK1L"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "tags = set([\"VERB\",\n",
        "    \"NOUN\",\n",
        "    \"PRON\",\n",
        "    \"ADJ\",\n",
        "    \"ADV\",\n",
        "    \"NUM\"])\n",
        "\n",
        "def processString(txt):\n",
        "  specialChars = \"!#$%^:;-+&*()?.,\"\n",
        "  for specialChar in specialChars:\n",
        "    txt = txt.replace(specialChar, ' ')\n",
        "  return txt.lower()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnBClDyF9xrH",
        "outputId": "5652c028-829b-4759-bc2e-c611dec9a441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "import math\n",
        "from sklearn.preprocessing import normalize\n",
        "import time\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "import requests\n",
        "import spacy\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def stem_and_lemmatize(df, column ='text', stopword=True, language='english'):\n",
        "\n",
        "      stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
        "      stopwords = set(stopwords_list.decode().splitlines())\n",
        "\n",
        "      standard = nlp(processString(df))\n",
        "      docs = \" \".join([token.lemma_ for token in standard])\n",
        "      docc = docs.strip()\n",
        "      tagged = pos_tag(docc.split(),tagset=\"universal\")\n",
        "      words = [word for word, tag in tagged if tag in ['NOUN', 'VERB', 'ADJ', 'ADV'] and len(word) > 1 and word not in stopwords]\n",
        "\n",
        "      return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9LegH-_BpTu"
      },
      "outputs": [],
      "source": [
        "onta.standards = onta.standards.fillna('-')\n",
        "onta['standards'] = onta['standards'].apply(lambda x: x.lower())\n",
        "onta['standards']= onta['standards'].apply(processString)\n",
        "onta['tokenized']= onta['docustandardsment'].apply(lambda x: stem_and_lemmatize(x, column=column_name))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um9RMnJDBpWY"
      },
      "outputs": [],
      "source": [
        "ausdf.standards = ausdf.standards.fillna('-')\n",
        "ausdf['standards'] = ausdf['standards'].apply(lambda x: x.lower())\n",
        "ausdf['standards']= ausdf['standards'].apply(processString)\n",
        "ausdf['tokenized']= ausdf['docstandardsument'].apply(lambda x: stem_and_lemmatize(x, column=column_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7qMYQnkDyCb"
      },
      "outputs": [],
      "source": [
        "ausdf.to_excel('ausdf_final.xlsx')\n",
        "onta.to_excel('onta_final.xlsx')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "7703f9b00bbd9eabf6b5dbd8e2e28e3def65628c7caa8b5bd7a5214360b62133"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('my_python_env': conda)",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}